# Into_Film--Pix2Pix-and-SinGAN

Using pix2pix and SinGAN to get into the movie

## Pix2Pix
![image](https://user-images.githubusercontent.com/69034494/163197685-38160454-dd39-4fd5-abc5-7da9f85255e2.png)
 * This research is inspired by this paper, which aims to investigate how to build and train a conditional generative adversarial network (cGAN) called pix2pix, which learns image translation training from input images to output images.

 * pix2pix can be used for a wide range of tasks, including compositing photos from tagged maps, generating color photos from black and white images, converting Google Maps photos to satellite aerial images, or converting sketches to photos.


## SinGAN
![image](https://user-images.githubusercontent.com/69034494/163198062-c7fc22c6-34ee-4e69-8b20-378ae6cb03d0.png)
 * The second part of this research uses SinGAN for image fusion. The authors of this paper are from Technion and Google.

 * In general, training GANs to generate images requires at least thousands of training data to have good results. SinGAN can train a generative model with only a single natural image.



## Reference

[Pix2Pix: Image-to-image translation with a conditional GAN](https://www.tensorflow.org/tutorials/generative/pix2pix)--Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros

[SinGAN: Learning a Generative Model from a Single Natural Image](https://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf)--Tamar Rott Shaham, Tali Dekel, Tomer Michaeli 
